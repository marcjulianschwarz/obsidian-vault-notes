---
tags: [paper, keep, note]
---

# Attention Is All You Need

## Short summary

Until now there have been mostly complex [[Recurrent Neural Network]]s or [[Convolution]]al [[Neural Network]]s that inlcude an encoder and a decoder.
The best methods also use an attention mechanism.

The paper suggests a new network architecture that only uses the [[Attention]] mechanism and leaves out any recurrence and convolutions.

## More details

### Model Architecture

The [[Transformer Architecture]] model has two main parts, the [[Encoder]] and the [[Decoder]].

The [[Multi-Head Attention]]

[[The Scaled Dot-Product Attention]]


![[Transformer Model Architecture.canvas]]

![[Bildschirm­foto 2022-12-25 um 16.40.53.png]]
![[Bildschirm­foto 2022-12-25 um 16.50.19.png]]

## Other References

- https://www.youtube.com/watch?v=iDulhoQ2pro
- https://www.youtube.com/watch?v=TQQlZhbC5ps
- https://www.youtube.com/watch?v=rBCqOTEfxvg
