---
uni-module: ISSP
tags: MOC
---
# MOC - Introduction to Statistics and Statistical Programming


[[Scale of measure]]

A finite data sequence $x_1,...,x_n$ with real data $x_i$ has values (unique values, sometimes called [[Item Expression]]) $\xi_1<...<\xi_k$ where $k\leq n$ because the same value can be measured multiple times.

We interpret [[stochastisch unabhängig|Stochastic Independence]] as causal independence, which means that we will look at the progression of data sequences to determine if they are independent of another.

Use [[Dichotomisation]] to make data usable for [[Runs test for long 01-sequence]] which determines whether a data sequence is random or not.

We want to be able to infer the underlying random mechanism of a data sequence. [[Ordered Data Sequence]]s makes it a bit easier as the order usually doesn't play a role.

- Definition of [[Relative Häufigkeit#Relative Frequency for Interval|Relative Frequency for Intervals]]
- Relative Frequencies always [[gleichmäßige Konvergenz|uniform convergent]] for every interval
- We use the limit of [[Relative Häufigkeit|Relative Frequency]] to define the probability.
- [[Relative Häufigkeit#Estimate error|Estimate Error]] → Unknown probability mostly (has to be quantified by percentage) in [[Confidence Interval]] $\left[h_{n}(I)-1 / \sqrt{n}, h_{n}(I)+1 / \sqrt{n}\right]$

We can use a [[Barplot]] or [[Histogram]] to plot frequencies of items.
We can describe the probability for every interval in an [[Empirical cumulative distribution functions|ECDF]].

> [!TIP]- [[Relative Häufigkeit|Relative Frequency]] convergence
>
> - Coint Tossing where relative and interval frequencies always converge to $\frac{1}{2}$
> - Arrival times in $[0,1]$
>
> ![[Bildschirmfoto 2022-08-19 um 12.00.54.png]]

[[Verteilungsfunktion|CDF]] to [[Wahrscheinlichkeitsdichte|PDF]] relation.

→ Infer it from the data (Calculate every p-quantile)




## Chapter 3 - Random Variables

For real random [[Zufallsvariable|Random Variables]] we mostly have events as intervals or sets (in this case they are generated by repeated application of unions and complementations of intervals).
The set of all these intervals is called the [[Borel Sigma Algebra]] of $\mathbb{R}$ on which we can define the [[Wahrscheinlichkeitsmaß|Probability Measure]] which goes to $[0,1]$ from an event out of the [[Borel Sigma Algebra]].

$$
F_Y(t):=\mathbb{P}(Y \leq t)=\mathbb{P}\left(Y^{-1}((-\infty, t])\right)
$$

[[Indikatorfunktion|Identity Map]]
The probability of the identity map for $t$ is $F(t)$.

Linear Transformation
$$F_Z(t)=P(a+bY\leq t)=P(Y\leq (t-a)/b)=F_Y((t-a)/b)$$
where
$$Z=a+bY$$
ECDF:
Let there be a data sequence with [[Item Expression]] (ordered values) $\zeta_1<...<\zeta_k$ and frequencies $n_1,...,n_K$.

$$P(Y\leq t)= \sum_{j:\zeta_j\leq t}{P(Y=\zeta_j)}=F_n(t;x)$$

[[p-Value]]
[[Statistical Simulation]]


## Chapter 5 - Estimators

We would like to extend [[Verteilungsfunktion|CDFs]] to vector valued items. That way we can for example repeat the same experiment multiple times.

So we describe such a repeated experiment with a [[Zufallsvariable|Random Variable]] $X$ where every $X_i$ is a specific observation of that experiment.

We can define the [[Verteilungsfunktion|CDF]] like this:

$$F\left(t_1, \ldots, t_n\right)=\mathbb{P}\left(X \in\left(-\infty, t_1\right] \times \ldots \times\left(-\infty, t_n\right]\right)=\mathbb{P}\left(X_1 \leq t_1, \ldots, X_n \leq t_n\right)$$

If all random variables are [[stochastisch unabhängig|independent]] we can write:
$$\mathbb{P}\left(X_{1} \leq t_{1}, \ldots, X_{n} \leq t_{n}\right)=F\left(t_{1}\right) \cdot \ldots \cdot F\left(t_{n}\right)$$
This also [[Implikation|implies]]
$$\mathbb{E}\left(X_i X_j\right)=\mathbb{E}\left(X_i\right) \mathbb{E}\left(X_j\right) \text { for } i \neq j$$
which [[Implikation|implies]] uncorrelatedness.

We can use [[Standard Estimators]] to estimate [[Erwartungswert|Expectation]], [[Varianz|Variance]] and [[Verteilungsfunktion|CDF]] from a data sample.

## Chapter 6 - Gauß Test

> [!TIP]- General
> [[Null Hypothesis]] and Alternative
>
> - [[Type 1 Error]]
> - [[Type 2 Error]]
> - Requirement: Type 1 and 2 Error probability $\alpha\in(0,1)$ should be small.
>
> However typically $n$ isnt large enough which is why you often have to give up the requirement of a low [[Type 2 Error]].
>
> We can use a [[Power Function]] to describe a tests error rates and power.

- [[Test Statistic]]
- [[Gauß Test]]
- [[Planning Test Size]]

> [!TIP]- Robustness
>
> Test Statistic (standardized mean or other metrics/estimators) needs to be normally distributed. → [[Zentraler Grenzwertsatz]].
>
> For any arbitrary [[Verteilungsfunktion|CDF]] with [[Erwartungswert|Expectation]] $\mu$ and [[Varianz|Variance]] $\sigma^2$ we can apply the [[Zentraler Grenzwertsatz|Central Limit Theorem]].
> Thus the [[Standard Estimators|estimated mean]] is normally distributed and we can estimate $\sigma^2$ from a sample.
>
> → Apply [[Gauß Test]]
>
> This strategy also works for other [[Standard Estimators]] and similar tests can be developed.

> [!TIP] General Confidence Intervals for Gauß Tests
> We have with probability $1-\alpha$:
>
> $$
> \begin{gathered}
> \mu \in\left(-\infty, \bar{X}_{(n)}+z_{1-\alpha} \frac{\sigma}{\sqrt{n}}\right], \quad \mu \in\left[\bar{X}_{(n)}-z_{1-\alpha} \frac{\sigma}{\sqrt{n}}, \infty\right) \\
> \mu \in\left[\bar{X}_{(n)}-z_{1-\alpha / 2} \frac{\sigma}{\sqrt{n}}, \bar{X}_{(n)}+z_{1-\alpha / 2} \frac{\sigma}{\sqrt{n}}\right]
> \end{gathered}
> $$


## Chapter 11 - Distribution Parameters cont'd

[[Plug-In-Method]]
[[Maximum Likelihood Estimation]]

## Chapter 12 - Curve Fitting

- We have a data sequence and want to find a function that best describes it.
- "Best describing" can be quantified using an error metric.

> [!NOTE]- Relative Error
> ![[Relative Error]]

We now want to find or choose a function that minimizes this error. This method is also known as the **Method of Least Deviation**.
This method doesn't necessarily result in a unique minimal error (if at all minimal error) as often only numerical solutions exist.

This method leads to [[ISSP Chapter 12.2 Linear Model and the Least Squares Method|Least Squares Method]] which is a popular way of solving the described problem.

$\hat{f}$ is also called the _regression function_.

The [[Residuum]] can be used to study the underling random mechanisms.

Definition of the problem
We can choose a function $f$ from a set of all possible polynomials.

- [[Linear Model]]
- [[Exponential Growth]]
- [[Allometric Growth]]

**Least Squares**
This method uses a [[Linear Model]] and the _least squares_ error function.

$$\|x-A(t) \cdot \gamma\|_2$$
See [[Mean Squared Error]] for more details.

Solutions depend on the complexity of the model which is mostly determined by the dimensions of $\gamma$ :

- [[Arithmetic Mean]] for $s=1$
- [[ISSP Chapter 12.3 Simple Linear Regression|Simple Linear Regression]] for $s=2$
- Polynomial Regression for $s=d+1$
- Multiple Linear Regressions for $s=d+1$

[[Linear Gaussian Model]]

**Geometric View**

From the geometric view, the least squares method can be understood as an orthogonal projection

> [!NOTE]- Illustration
> ![[Bildschirmfoto 2022-09-22 um 19.40.13.png]]

[[Linear Regression#Simple Linear Regression]]

The method is often numerically stable, even for non-linear models.



## Chapter 17 - Limit Theorems and Simulations

In this first subchapter we learn about the [[Starkes Gesetz der großen Zahlen|Strong Law of Large Numbers]] which we can use to define the [[Glivenko-Cantelli]] theorem.

The fundamental theorems become important when we want to talk about Kolmogorov in the next subchapter.

In this subchapter we define a general goodness of fit test that can test wether the unknown [[Wahrscheinlichkeitsmaß|Distribution]] of a data sequence euqals some distribution $G$.

We can use this general approach to definde the [[Kolmogorov Smirnov Test]] which uses the [[Kolmogorov Distribution]] for its [[Test Statistic]].

Furthermore we can use the [[Lilliefors Test of Normality]] to determine whether a data sequence from an unknown distribution falls into the class of normal distributions,
